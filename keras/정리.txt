keras란?
deep learning의 이해를 돕기위해 만든 간단한 형태의 library함수

hyper parameter :  deep learning을 수행할때 결과값에 지대한 영향을 끼치는 중요한 매개변수(parameter)
1. node, layer : 모델의 구조, layer는 층, node는 각 층의 방 이라고 설명가능
                이때 node, layer의 개수를 정하는데 정확한 룰은 없으며 본인의 경험과 지식, 그리고 논문참조 등등을 통해 정하게 된다.
                node,layer의 수가 일정 이상 넘어가면 모델의 정확도에 큰 의미가 없으며 오히려 정확도가 떨어지는 모습도 보이기 때문에 (과적합)
                데이터의 시각화를 통해 데이터의 복잡도를 보고 모델을 만드는 것이 의미가 있을지 한번 해보고 싶다.
2. epochs, batch_size : 각각 시행횟수, 한번에 집어넣는 데이터 개수를 의미한다.
                        이때 역시 시행횟수가 높아지면 과적합에 의해 정확도가 떨어지는 모습을 보이며, 한번에 넣는 데이터의 개수도 한개씩 넣는다고 꼭 좋은 정확도를 보이는 모습도 보이지 않았다.
                        이 또한 많은 시행착오를 통해 익숙해지지 않을까 생각한다.

연산횟수
layer의 개수가 n개, 각 레이어의 노드 개수가 a(i) (i는 1부터 n) 이라고 하면
sigma((a(i) + 1) * a(i+1))* data개수 / batch_size  * epochs

곱셈이 많고 그 중 data개수가 포함되어 있기 떄문에 데이터의 크기가 커질수록 연산시간이 길어져 다른것으로 조정이 필요하다

loss(손실) : 실제 데이터와 예측한 모델의 데이터를 비교하여 얼마나 손실이 났는지 계산하는 연산식, mse, mae등등 다양한 식이 존재한다. 
metrics(측정항목) : 추가로 출력할 데이터값들을 선택하여 출력가능, 모델의 정확도를 측정해주는 지표들을 주로 넣는다.

데이터의 분할
적합하는데 사용한 훈련데이터로 test를 한다면 당연하게도 높은 정확도를 보인다. 같은 이유로 매번 적합시 평가를 하는 지표 또한 훈련데이터로 하면 매번 정상적인 평가를 할 수 없다.
따라서 데이터를 train data, test data, validation data로 나누어 모델을 훈련한다.
이때 데이터를 나누는 %나 나누는 방식은 다양한 방식이 있다.

각 데이터는 변수를 하나만 가지고 있지 않고 다양한 개수의 변수를 가지고 있다.
이때 이 데이터를 통해 여러개의 결과값을 적합해낼수 있다.
다 : 다, 다 : 1, 1 : 다, 1 : 1 등으로 표현할 수 있고 당연하게도 다양한 데이터로 한개의 데이터를 예측하는것이 결과가 잘 나올것이다.

인공신경망의 경우 y = wx + b의 형태로 각 층을 계산하기 때문에 복잡한 형태의 데이터 구조는 한번에 파악하기 어렵다. 이때 multi - layer - perceptron (mlp)는 딥러닝의 주효한 기술로
이러한 y = wk + b의 계산을 행하는 layer를 여러개 쌓아 복잡한 데이터도 분석할수 있게 한것이 딥러닝이다.
반대로 데이터의 구조가 단순할 경우 과한 레이어와 노드는 오히려 복잡한 구조를 다 찾아보려 하다가 적합도가 더 내려가거나 정확한 모델을 못찾는 경우가 많다.
예시로 주시는 단순한 데이터는 레이어가 2개일때 젤 적합이 잘되는것으로 보였다.

딥러닝은 순차형 모델과 함수형 모델로 구분되어있다.
순차형은 sequential함수에 레이어와 노드를 차곡차곡 쌓는것이고,
함수형은 직접 Dense함수끼리 연결시켜준다. 이때 함수형으로 ensemble을 사용할수 있다.

ensemble이란?
여러 데이터 array를 한 array로 합치지 않고 모델을 중간에 레이어를 합쳤다가 분리하여 계산하는 방식
다양한 활용이 가능하여 보인다.
1. 필요한 데이터들이 따로따로 주어졌는데 데이터의 크기가 너무 커서 데이터를 합치는 행위 자체가 속도적인 손해가 될경우에 ensemble을 사용하여
모델의 손실없이 그냥 사용 가능하다. input, output들쪽 가지를 input, output에 해당하는 노드만 남기고 추가하지 않고 middle(몸통)에서 모델을 형성하면 데이터를합친것과 다를것이 없다.
2. 각 데이터의 형태가 다르지만 이 두개가 연관성이 있어 결과값에 영향을 끼치는경우 데이터를 분리해서 따로 쓰는 노드가 있는것이 좋을것으로 보인다.
 예를 들어 한 데이터는 2차함수의 형태를 띄고 한 데이터는 1차함수의 데이터형태를 띌때 모든 노드를 이 두 데이터가 전부 같이 쓴다면 노드가 잘 적합하지 않을것으로 생각된다.
concatenate함수를 통해 두 레이어를 합쳐준다.


early stopping
모델을 적합중에 모델이 더이상 좋게 적합되지 않는다면 진행을 멈추는 방식
최대로 적합하였을때 더 적합을 진행한다면 과적합이 발생할수 있어 이를 막기위한 방법이다.
다만 이미 최대로 적합한 후, 몇번 더 수행하여야 멈출수 있으므로 과적합이 어느정도 발생한 후에 멈추게 되며
patience (모델이 적합되지 않는 유예 횟수)의 값이 너무 적다면 제데로 적합되지 않았는데 멈춰버리는 경우가 발생한다.
따라서 early stopping만을 사용한다면 어느정도의 과적합을 감수하고 patience를 "적당히" 크게 만들어줄 필요성이 있다.

DNN RNN CNN

DNN = Deep Neural Network (심층 신경망)

CNN = Convolutional Neural Network (합성곱 신경망)

RNN = Recurrent Neural Network (순환 신경망)
특정 데이터를 기준으로 연속되는 데이터 (시간순 등)에서 다음 데이터를 예측할수있게 하기
이때 이 연속되는 데이터가 범위가 정해져있고 조건이 맞춰진다면 예측이 가능
RNN의 가장 대표적인 방법 LSTM (Long Short Term Memory model)
최근에는 TF 2.0 + keras
레거시한 머신러닝모델은 속도가 훨신 빠름

keras 추가방법 model.add(Dense ..) -> model.add(LSTM ..) 로 바꾼후 input output 데이터값 건드리면 됨

스칼라 :  단일 변수, 숫자하나
벡터 : 스칼라들의 배열
행렬 : 벡터의 배열
텐서 : 행렬의 배열 

데이터 분류법 : (batch_size, time_step, feature) 의 형태로 분류
batch_size는 분류된 데이터의 개수, time_step은 분류한 시계열 데이터의 길이, feature은 데이터가 가지고있는 변수개수

ex) 실제 시간에 따라 1,2,3,4,5,6,7 순의 데이터가 있다고 하면

    time_step
 b  [1] [2] [3] | [4]
 a  [2] [3] [4] | [5]
 t  [3] [4] [5] | [6]
 c  [4] [5] [6] | [7]
 h
이런식으로 분류할 수 있고
이때 time_step = 3, feature = 1로 되어 batch_size = 4 가 되었다.


RNN종류 = simpleRNN , GRU, LSTM
각각의 parameter 계산법

simpleRNN : (input_dim + output + 1) * output
GRU : (input_dim + output + 1) * output * 3
LSTM : (input_dim + output + 1) * output * 4

각각의 계산식은 비슷한테 곱해주는 상수가 다른 이유?
전체적인 틀은 같지만 내부에서 곱하는 weight의 갯수가 다름 => simpleRNN -> GRU -> LSTM 으로 갈수록 내부적으로 복잡한 계산구조를 가짐

따라서 복잡한 데이터를 분석할수록 내부적으로 복잡하게 계산을 하는 LSTM이 좀 더 잘 분석할 것으로 보인다.

