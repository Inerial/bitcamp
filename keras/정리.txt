keras란?
deep learning의 이해를 돕기위해 만든 간단한 형태의 library함수

hyper parameter :  deep learning을 수행할때 결과값에 지대한 영향을 끼치는 중요한 매개변수(parameter)
1. node, layer : 모델의 구조, layer는 층, node는 각 층의 방 이라고 설명가능
                이때 node, layer의 개수를 정하는데 정확한 룰은 없으며 본인의 경험과 지식, 그리고 논문참조 등등을 통해 정하게 된다.
                node,layer의 수가 일정 이상 넘어가면 모델의 정확도에 큰 의미가 없으며 오히려 정확도가 떨어지는 모습도 보이기 때문에 (과적합)
                데이터의 시각화를 통해 데이터의 복잡도를 보고 모델을 만드는 것이 의미가 있을지 한번 해보고 싶다.
2. epochs, batch_size : 각각 시행횟수, 한번에 집어넣는 데이터 개수를 의미한다.
                        이때 역시 시행횟수가 높아지면 과적합에 의해 정확도가 떨어지는 모습을 보이며, 한번에 넣는 데이터의 개수도 한개씩 넣는다고 꼭 좋은 정확도를 보이는 모습도 보이지 않았다.
                        이 또한 많은 시행착오를 통해 익숙해지지 않을까 생각한다.

연산횟수
layer의 개수가 n개, 각 레이어의 노드 개수가 a(i) (i는 1부터 n) 이라고 하면
sigma((a(i) + 1) * a(i+1))* data개수 / batch_size  * epochs

곱셈이 많고 그 중 data개수가 포함되어 있기 떄문에 데이터의 크기가 커질수록 연산시간이 길어져 다른것으로 조정이 필요하다

loss(손실) : 실제 데이터와 예측한 모델의 데이터를 비교하여 얼마나 손실이 났는지 계산하는 연산식, mse, mae등등 다양한 식이 존재한다. 
metrics(측정항목) : 추가로 출력할 데이터값들을 선택하여 출력가능, 모델의 정확도를 측정해주는 지표들을 주로 넣는다.

데이터의 분할
적합하는데 사용한 훈련데이터로 test를 한다면 당연하게도 높은 정확도를 보인다. 같은 이유로 매번 적합시 평가를 하는 지표 또한 훈련데이터로 하면 매번 정상적인 평가를 할 수 없다.
따라서 데이터를 train data, test data, validation data로 나누어 모델을 훈련한다.
이때 데이터를 나누는 %나 나누는 방식은 다양한 방식이 있다.

각 데이터는 변수를 하나만 가지고 있지 않고 다양한 개수의 변수를 가지고 있다.
이때 이 데이터를 통해 여러개의 결과값을 적합해낼수 있다.
다 : 다, 다 : 1, 1 : 다, 1 : 1 등으로 표현할 수 있고 당연하게도 다양한 데이터로 한개의 데이터를 예측하는것이 결과가 잘 나올것이다.

인공신경망의 경우 y = wx + b의 형태로 각 층을 계산하기 때문에 복잡한 형태의 데이터 구조는 한번에 파악하기 어렵다. 이때 multi - layer - perceptron (mlp)는 딥러닝의 주효한 기술로
이러한 y = wk + b의 계산을 행하는 layer를 여러개 쌓아 복잡한 데이터도 분석할수 있게 한것이 딥러닝이다.
반대로 데이터의 구조가 단순할 경우 과한 레이어와 노드는 오히려 복잡한 구조를 다 찾아보려 하다가 적합도가 더 내려가거나 정확한 모델을 못찾는 경우가 많다.
예시로 주시는 단순한 데이터는 레이어가 2개일때 젤 적합이 잘되는것으로 보였다.

딥러닝은 순차형 모델과 함수형 모델로 구분되어있다.
순차형은 sequential함수에 레이어와 노드를 차곡차곡 쌓는것이고,
함수형은 직접 Dense함수끼리 연결시켜준다. 이때 함수형으로 ensemble을 사용할수 있다.

ensemble이란?
여러 데이터 array를 한 array로 합치지 않고 모델을 중간에 레이어를 합쳤다가 분리하여 계산하는 방식
다양한 활용이 가능하여 보인다.
1. 필요한 데이터들이 따로따로 주어졌는데 데이터의 크기가 너무 커서 데이터를 합치는 행위 자체가 속도적인 손해가 될경우에 ensemble을 사용하여
모델의 손실없이 그냥 사용 가능하다. input, output들쪽 가지를 input, output에 해당하는 노드만 남기고 추가하지 않고 middle(몸통)에서 모델을 형성하면 데이터를합친것과 다를것이 없다.
2. 각 데이터의 형태가 다르지만 이 두개가 연관성이 있어 결과값에 영향을 끼치는경우 데이터를 분리해서 따로 쓰는 노드가 있는것이 좋을것으로 보인다.
 예를 들어 한 데이터는 2차함수의 형태를 띄고 한 데이터는 1차함수의 데이터형태를 띌때 모든 노드를 이 두 데이터가 전부 같이 쓴다면 노드가 잘 적합하지 않을것으로 생각된다.
concatenate함수를 통해 두 레이어를 합쳐준다.


early stopping
모델을 적합중에 모델이 더이상 좋게 적합되지 않는다면 진행을 멈추는 방식
최대로 적합하였을때 더 적합을 진행한다면 과적합이 발생할수 있어 이를 막기위한 방법이다.
다만 이미 최대로 적합한 후, 몇번 더 수행하여야 멈출수 있으므로 과적합이 어느정도 발생한 후에 멈추게 되며
patience (모델이 적합되지 않는 유예 횟수)의 값이 너무 적다면 제데로 적합되지 않았는데 멈춰버리는 경우가 발생한다.
따라서 early stopping만을 사용한다면 어느정도의 과적합을 감수하고 patience를 "적당히" 크게 만들어줄 필요성이 있다.

DNN RNN CNN

DNN = Deep Neural Network (심층 신경망)

CNN = Convolutional Neural Network (합성곱 신경망)

RNN = Recurrent Neural Network (순환 신경망)
특정 데이터를 기준으로 연속되는 데이터 (시간순 등)에서 다음 데이터를 예측할수있게 하기
이때 이 연속되는 데이터가 범위가 정해져있고 조건이 맞춰진다면 예측이 가능
RNN의 가장 대표적인 방법 LSTM (Long Short Term Memory model)
최근에는 TF 2.0 + keras
레거시한 머신러닝모델은 속도가 훨신 빠름

keras 추가방법 model.add(Dense ..) -> model.add(LSTM ..) 로 바꾼후 input output 데이터값 건드리면 됨

스칼라 :  단일 변수, 숫자하나
벡터 : 스칼라들의 배열
행렬 : 벡터의 배열
텐서 : 행렬의 배열 

데이터 분류법 : (batch_size, time_step, feature) 의 형태로 분류
batch_size는 분류된 데이터의 개수, time_step은 분류한 시계열 데이터의 길이, feature은 데이터가 가지고있는 변수개수

ex) 실제 시간에 따라 1,2,3,4,5,6,7 순의 데이터가 있다고 하면

    time_step
 b  [1] [2] [3] | [4]
 a  [2] [3] [4] | [5]
 t  [3] [4] [5] | [6]
 c  [4] [5] [6] | [7]
 h
이런식으로 분류할 수 있고
이때 time_step = 3, feature = 1로 되어 batch_size = 4 가 되었다.


RNN종류 = simpleRNN , GRU, LSTM
각각의 parameter 계산법

simpleRNN : (input_dim + output + 1) * output
GRU : (input_dim + output + 1) * output * 3
LSTM : (input_dim + output + 1) * output * 4

각각의 계산식은 비슷한테 곱해주는 상수가 다른 이유?
전체적인 틀은 같지만 내부에서 곱하는 weight의 갯수가 다름 => simpleRNN -> GRU -> LSTM 으로 갈수록 내부적으로 복잡한 계산구조를 가짐

따라서 복잡한 데이터를 분석할수록 내부적으로 복잡하게 계산을 하는 LSTM이 좀 더 잘 분석할 것으로 보인다.

첫 LSTM layer에 input_shape를 (3,1)을 넣었을때 return_sequences = True를 주지 않으면 리턴시키는 값의 shape가 (batch_size, time_step, feature)의 형태가 아닌
기존의 (batch_size, output)의 형태로 나오게 설정되어있다.
연속으로 LSTM을 쓰기 위해서는 return_sequences=True를 써주어야 한다. 반대로 Dense를 쓰기 위해서는 다시 false값을 주어야한다(디폴트값)

one layer에 one bias  다양한 bias가 필요하다면 LSTM을 더 만드는 것이 좋다.

parameter 계산은 여전히 (input_dim + output + 1) * output * 4 이다.
이떄 두번째 이후 input은 이전 LSTM의 output과 같다.

return_sequences=True값을 준다면 LSTM의 shape형태로 출력되므로 output_shape = (none, 원래 준 time_step, output크기(output노드수 == 다음레이어의 input노드수))

데이터가 단순하고 적어서 모델을 짜기가 편하고 적합이 빠르게 된다.
LSTM보다 dense가 더 좋은 모델이라고 할수는없다.(데이터에 영향을 크게 받으므로)


standardization  :  표준화 => 표준정규분포로 만들어줌  == (데이터 - 평균) / 표준편차
minmax(nomalization) : 정규화 => 구간을 직접 설정하여 구간에 맞추어 데이터를 바꾸는것 == (데이터 - 최소값)/(최대값 - 최소값) (범위가 0~1일때)


최소 최대값 == 데이터의 범위
범위를 min ~ max로 scaling 하면 가중치가 한쪽으로 치우쳐 잘못 계산되는 것을 줄일수있다.()

이 두개를 하나만 사용, 혹은 적절히 두개를 사용하여? 데이터를 깔끔하게 만들어준다.

이때 표준화를 사용하면 데이터의 개수가 너무 많을경우 계산이 너무 복잡해질수 있어 정규화를 사용하는것 같다?

정규화는 데이터의 이상치를 솎아내기 좋다.

어차피 x 데이터 간의 비율은 변하지 않기 때문에 y의 값은 건드리지 않아도 된다.


 	종류	설명
1	StandardScaler	기본 스케일. 평균과 표준편차 사용
2	MinMaxScaler	최대/최소값이 각각 1, 0이 되도록 스케일링
3	MaxAbsScaler	최대절대값과 0이 각각 1, 0이 되도록 스케일링
4	RobustScaler	중앙값(median)과 IQR(interquartile range) 사용. 아웃라이어의 영향을 최소화



지금까지 배운 전처리
train_test_split
scaler

이상치를 제거하고 전처리를 해주는것이 좋다.


lstm 시계열 데이터 분할

1 2 3 4 5 6 7 8 9 10
=>
1 2 3 4 5
2 3 4 5 6
3 4 5 6 7
4 5 6 7 8
5 6 7 8 9
6 7 8 9 10    형태로 분할해줌 

batch_size : len(data) - time_step + 1
time_step : time_step
feature : 1 (시계열데이터의 종류갯수)

python에서 list, array의 index 분할 방법
list[1: , :4 , :] 방식

똑같이 train_test_split, validation_split 등등 그대로 먹힘 
 == 행기준으로 그대로 수행되므로

모델의 저장 및 불러오기

저장:
model.save("./폴더/파일명.h5")
기본적으로 라이브러리에 들어가있음 (따로 선언 x)

불러오기:
from keras.models import load_model
model = load_model("같은 주소")
선언 필요
선언한 모델에 추가 입력 가능 (단 layer 이름중복 주의)



history 가져오기
기본적으로 model.fit()함수는 적합내용 데이터를 리턴해준다
hist = model.fit()  이떄 hist는 dict 

hist.history 안에 loss, metrics 값들이 들어있어 그것을 이용하여 그래프를 그리면된다.
(validation지정시 해당 값도 출력가능)

이때 import matplotlib.pyplot as plt 라는 라이브러리가 무난한 시각화 라이브러리
자세한 시행방법은 46번파일과 구글링을 통해 연습하도록 하자 (매우 다양함)



tensorboard

keras.models 의 tensorboard함수를 통해 그래프를 그릴수도 있다.
이는 선택사항, 그래프는 이쁘지만 귀찮은 과정


from keras.callbacks import TensorBoard
tb_hist = TensorBoard(log_dir = '.\graph', histogram_freq = 0, 
                      write_graph = True, write_images = True,)

이렇게 tb_hist를 지정해준 다음
fit시 callbacks parameter에 tb_hist를 넣어주면 된다.

log_dir 경로의 폴더에 관련 데이터들이 들어가지므로 비어있는 폴더(없으면 생성한다.)

두번 이상 연속으로 시행하면 데이터들이 계속 덮어씌워지며 겹쳐진다. (지워줘야함)

제데로 돌아갔다면
cmd에서 데이터가 들어간 폴더 찾아 들어간후 tensorboard --logdir=. 입력
그후 인터넷 주소에 http://127.0.0.1:6006/#scalars 입력시 그래프가 뜸




분류모델

분류모델이란? 결과로 나오는 y값이 연속형 자료(몸무게와 같이 연속(순서)됨을 보일수 있는 자료)
가 아닌 범주형 자료(오렌지, 사과, 배 와 같이 연관성은 있으나 연속, 순서를 보일수 없는 자료)
로 이루어져 있는 데이터를 분류하는 방법을 분류모델이라고 한다.

분류모델에서 사용하는 loss 값은 두가지로 고정!! 이때 model의 마지막 output에 들어가는 activation도 고정
이진분류 : binary_crossentropy   - activation : "sigmoid"
다항분류 : categorical_crossentropy - activation : "softmax"

중간에 들어가는 activation은 대부분의 경우 "relu"가 유효 하지만
데이터의 feature가 1이면서 이진분류 "sigmoid"로 끝나는 경우
데이터가 relu 계산에 의해 0이 되버리면서 노드가 죽어버리는 오류가 발생한다.
특히 sigmoid함수는 데이터가 0 이 들어가면 무조건 0.5가 나오기 때문에 제데로된 값이 나오지 않는다.

데이터의 크기가 작을수록 이러한 현상이 자주 발생할것으로 보이며 "elu" 를 사용하면
이러한 문제점을 해결할 수 있었다.


다항분류의 경우 데이터의 크기에 상관없이 relu도 잘 먹히는 모습을 보였다.
아마도 one_hot_encoding을 통해 y의 크기가 커져서 그런것으로 보인다.

다항분류의 경우 y의 값을 one_hot_encoding을 통해 데이터를 전처리 해주어야 
loss 값으로 categorical_crossentropy를 사용할 수 있었다. (아니면 오류)

one_hot_encoding이란?
1~5사이의 값을 가지는 범주형 데이터가 있을때

1을 [1,0,0,0,0]
2를 [0,1,0,0,0]과 같이 변형해주어 행렬로 만들어주는 데이터 전처리방식

사용하는 함수는 keras.utils 에 있는 to_categorical(seq) 함수
이때 넣는 seq 배열을 one_hot_encoding방식으로 바꾸어 리턴해준다.
이때 seq배열의 max값을 찾아 (len(seq) , max+1) 만큼의 dim을 가진 행렬을 리턴
만약 데이터의 최소값이 0이 아니라면
음수는 배열마냥 반대편으로 넘어가 -1이 max값으로 변하게 되고
양수이면 하나도 사용하지 않는 열이 왼쪽에 만들어지게 된다.

이를 막기 위해 y의 최솟값을 기억해둔 다음
y에 min값을 빼서 인코딩을 한다면 자연스럽게 최솟값이 0이 되어 이러한 문제점이 없어지게 된다.
마지막 결과에 저장해둔 min값을 그대로 더해준다면 원래값으 그대로 다시 나오게 된다.

분류모델의 경우 확률이 가장 높게 나오는 쪽을 선택하면 된다.
(그래도 최대한 확률이 높게 분류해보도록 하자)



CNN   Conv2D

자르고 증폭시키며 사진의 특성을 찾아내는 방법
x의 shape는 4차원 텐서 (2D)
(batch_size, height, width, channels)

Conv2D의 매개변수
filters == output
kernel_size == 필터의 가로세로 크기
input_shape == 입력할 인풋 x의 shape (batch_size 제외 == 3차원텐서)
padding == 'valid' : 기본값 적용안하는것
            'same' : kernel_size값을 기반으로 0을 사진에 둘러준다.
                     상대적으로 적게 적합될 수 있는 valid에 비해 더 가생이 적합가능

strides == default 1, 필터를 쓰는 간격, 2를 쓰면 필터를 두칸마다 사용 데이터의 크기가 크면 계산을 줄일수 있다.



MaxPooling2D : 받은 데이터에 pool_size만큼 묶어 가장 큰 값을 하나 꺼냄
               데이터의 크기를 줄이거나 특정 데이터를 강조할수 있다.
== 비슷한 것으로 AveragePooling2D가 존재 : 묶은 값들의 평균

Flatten : 위에서 내려온 3차원 텐서를 1차원 벡터로 변경시켜준다.
          == Conv2D종료 후 분류분석을 할때 데이터형식 변경

CNN의 parameter 계산법은
(kernel_height * kernel_width * input_channel + 1) * output_channel

1은 bias

filter로 사진을 조각조각 내면 그 너비만큼 channel이 곱연산으로 늘어난다.
(윗채널 + 1) * 아랫채널 의 방식으로 방식으로는 dense와 큰 차이가 없음

다만 한 노드가 나눈 사진 하나하나를 의미한다는점